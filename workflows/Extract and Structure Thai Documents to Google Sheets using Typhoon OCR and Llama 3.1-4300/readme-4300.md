Extract and Structure Thai Documents to Google Sheets using Typhoon OCR and Llama 3.1

https://n8nworkflows.xyz/workflows/extract-and-structure-thai-documents-to-google-sheets-using-typhoon-ocr-and-llama-3-1-4300


# Extract and Structure Thai Documents to Google Sheets using Typhoon OCR and Llama 3.1

### 1. Workflow Overview

This workflow automates the extraction and structuring of Thai language document data from PDFs into Google Sheets. It is designed for use cases involving official or governmental documents written in Thai, where text extraction is performed by Typhoon OCR, followed by semantic structuring into JSON using a large language model (Llama 3.1 via OpenRouter), and finally the structured data is saved to a Google Sheets spreadsheet for easy access and further processing.

The workflow is logically divided into these main blocks:

- **1.1 Input Reception:** Manual trigger starts the workflow, loading PDF files from a designated folder.
- **1.2 OCR Extraction:** Extract raw text from PDFs using Typhoon OCR via a Python command.
- **1.3 AI Processing:** Use Llama 3.1 to transform raw OCR text into a structured JSON object with predefined fields.
- **1.4 Data Parsing and Cleaning:** Parse the JSON output from AI, clean and normalize data into sheet-compatible format.
- **1.5 Storage:** Append the cleaned structured data into a Google Sheets document.

---

### 2. Block-by-Block Analysis

#### 2.1 Input Reception

- **Overview:** This block initiates the workflow manually and loads PDF documents from a local folder named `doc/` for processing.
- **Nodes Involved:**  
  - When clicking ‘Test workflow’  
  - Load PDFs from doc Folder

- **Node Details:**

  - **When clicking ‘Test workflow’**  
    - Type: Manual Trigger  
    - Role: Entry point for manual workflow execution  
    - Configuration: Default, no parameters  
    - Input: None  
    - Output: Triggers downstream nodes  
    - Edge cases: User must manually start; no files present in folder leads to no further processing.

  - **Load PDFs from doc Folder**  
    - Type: Read/Write File  
    - Role: Reads all PDFs from the `doc/` folder  
    - Configuration: File selector pattern `doc/*` to pick all files in the folder  
    - Input: Trigger from manual node  
    - Output: Each file as item for next node  
    - Edge cases: Folder empty or inaccessible; file read errors; non-PDF files might be present.

#### 2.2 OCR Extraction

- **Overview:** Extracts text content from the loaded PDF files using Typhoon OCR by running a Python command that calls Typhoon OCR API.
- **Nodes Involved:**  
  - Extract Text with Typhoon OCR

- **Node Details:**

  - **Extract Text with Typhoon OCR**  
    - Type: Execute Command  
    - Role: Runs Python script to extract OCR text from PDF files  
    - Configuration: Inline Python command setting environment variable `TYPHOON_OCR_API_KEY`, importing `typhoon_ocr` module, and executing OCR on file path from JSON `fileName` field  
    - Key Expressions: Uses `doc/{{$json["fileName"]}}` to locate file path dynamically  
    - Input: Files loaded from previous node  
    - Output: Raw OCR text in UTF-8 encoding sent downstream as `stdout` field  
    - Edge cases: Missing or invalid API key; Python environment issues; OCR API timeouts or failures; file path errors.

#### 2.3 AI Processing

- **Overview:** Converts the raw OCR text into a structured JSON object with predefined metadata fields using the Llama 3.1 model hosted on OpenRouter.
- **Nodes Involved:**  
  - OpenRouter Chat Model  
  - Structure Text to JSON with LLM

- **Node Details:**

  - **OpenRouter Chat Model**  
    - Type: LangChain LLM Chat Node  
    - Role: Runs chat completion on Llama 3.1 (Typhoon fine-tuned) via OpenRouter API  
    - Configuration: Model set to `scb10x/llama3.1-typhoon2-70b-instruct` with empty options  
    - Credentials: OpenRouter API key required  
    - Input: Not directly connected to upstream nodes; used as an AI model provider  
    - Output: Provides language model inference capability for the next node  
    - Edge cases: Authentication failure; API throttling; network issues.

  - **Structure Text to JSON with LLM**  
    - Type: LangChain Chain LLM Node  
    - Role: Sends prompt with OCR text to LLM to extract structured JSON fields  
    - Configuration: Prompt instructs model to extract 10 specific fields (book_id, date, subject, to, attach, detail, signed_by, signed_by2, contact, download_url) from OCR text supplied via expression `{{ $json["stdout"] }}`  
    - Input: Raw OCR text output from previous node  
    - Output: JSON text as LLM response  
    - Edge cases: Model hallucination; malformed JSON output; API timeout.

#### 2.4 Data Parsing and Cleaning

- **Overview:** Parses the JSON text generated by the LLM, removes formatting artifacts, and normalizes fields to a flat structure suitable for Google Sheets.
- **Nodes Involved:**  
  - Parse JSON to Sheet Format

- **Node Details:**

  - **Parse JSON to Sheet Format**  
    - Type: Code (JavaScript)  
    - Role: Cleans LLM response from markdown formatting, parses JSON, extracts nested contact fields, and returns a flat object  
    - Configuration: Runs JavaScript code to remove markdown code blocks like ```json, parse JSON, safely access nested contact info (phone, email, fax), and provide default empty strings for missing fields  
    - Input: LLM JSON text  
    - Output: Structured JSON object ready for sheet insertion  
    - Edge cases: Malformed JSON causing parsing errors (throws with detailed message); missing or nested contact info; empty fields.

#### 2.5 Storage

- **Overview:** Appends the structured data as a new row in a predefined Google Sheets spreadsheet.
- **Nodes Involved:**  
  - Save to Google Sheet

- **Node Details:**

  - **Save to Google Sheet**  
    - Type: Google Sheets  
    - Role: Appends data to a Google Sheet with specific column mapping  
    - Configuration:  
      - Document ID: Linked to a specific spreadsheet  
      - Sheet Name: Using gid=0 (first sheet)  
      - Columns: Mapped explicitly from parsed JSON keys (e.g., `to`, `date`, `book_id`, `contact_phone`, etc.)  
      - Operation: Append new rows  
      - Matching Columns: `book_id` used for matching (though append operation is chosen)  
    - Credentials: Google Sheets OAuth2 required  
    - Input: Structured, cleaned JSON data  
    - Output: Confirmation or error from Sheets API  
    - Edge cases: Authentication errors; quota limits; schema mismatch; network issues.

---

### 3. Summary Table

| Node Name                    | Node Type                            | Functional Role                          | Input Node(s)               | Output Node(s)                | Sticky Note                                                                                         |
|------------------------------|------------------------------------|----------------------------------------|-----------------------------|------------------------------|---------------------------------------------------------------------------------------------------|
| When clicking ‘Test workflow’ | Manual Trigger                     | Manual start of workflow                | None                        | Load PDFs from doc Folder     |                                                                                                   |
| Load PDFs from doc Folder     | Read/Write File                    | Load PDF files from `doc/` folder      | When clicking ‘Test workflow’| Extract Text with Typhoon OCR |                                                                                                   |
| Extract Text with Typhoon OCR | Execute Command                    | Run Typhoon OCR Python script on PDFs  | Load PDFs from doc Folder    | Structure Text to JSON with LLM|                                                                                                   |
| OpenRouter Chat Model         | LangChain LLM Chat Node            | Provides LLM inference (Llama 3.1)     | None (used by chain node)    | Structure Text to JSON with LLM|                                                                                                   |
| Structure Text to JSON with LLM| LangChain Chain LLM Node          | Convert OCR text to structured JSON    | Extract Text with Typhoon OCR| Parse JSON to Sheet Format    |                                                                                                   |
| Parse JSON to Sheet Format    | Code (JavaScript)                  | Clean & parse JSON, normalize data     | Structure Text to JSON with LLM| Save to Google Sheet         |                                                                                                   |
| Save to Google Sheet          | Google Sheets                     | Append structured data to Google Sheet | Parse JSON to Sheet Format   | None                         |                                                                                                   |
| Sticky Note                  | Sticky Note                       | Descriptive note about workflow purpose| None                        | None                         | ## Thai OCR to Sheet\nThis workflow extracts Thai PDF text using typhoon-ocr, converts it to structured JSON using LLM, and saves the output to Google Sheets. Works with self-hosted n8n only. |

---

### 4. Reproducing the Workflow from Scratch

1. **Create Manual Trigger Node**  
   - Type: Manual Trigger  
   - Name: "When clicking ‘Test workflow’"  
   - No special parameters.

2. **Create Read/Write File Node**  
   - Type: Read/Write File  
   - Name: "Load PDFs from doc Folder"  
   - Configure file selector to `doc/*` to read all files in the folder.  
   - Connect output of Manual Trigger to this node.

3. **Create Execute Command Node**  
   - Type: Execute Command  
   - Name: "Extract Text with Typhoon OCR"  
   - Command:  
     ```
     =python -c "import sys, os; os.environ['TYPHOON_OCR_API_KEY'] = '<YourTyphoonKey>'; from typhoon_ocr import ocr_document; sys.stdout.reconfigure(encoding='utf-8'); input_path = sys.argv[1]; text = ocr_document(input_path); print(text)" "doc/{{$json["fileName"]}}"
     ```  
   - Replace `<YourTyphoonKey>` with your actual Typhoon OCR API key.  
   - Connect output of "Load PDFs from doc Folder" to this node.

4. **Create LangChain LLM Chat Node**  
   - Type: LangChain LLM Chat Node (@n8n/n8n-nodes-langchain.lmChatOpenRouter)  
   - Name: "OpenRouter Chat Model"  
   - Model: `scb10x/llama3.1-typhoon2-70b-instruct`  
   - Credentials: Add OpenRouter API credentials.  
   - This node is used as an AI model provider; no direct input connection needed from upstream nodes.

5. **Create LangChain Chain LLM Node**  
   - Type: LangChain Chain LLM Node (@n8n/n8n-nodes-langchain.chainLlm)  
   - Name: "Structure Text to JSON with LLM"  
   - Prompt (Thai language instruction):  
     ```
     ข้อความด้านล่างนี้เป็นเนื้อหา OCR จากหนังสือราชการ กรุณาแยกหัวข้อสำคัญออกมาในรูปแบบ JSON:

     1. book_id: เลขที่หนังสือ
     2. date: วันที่ในเอกสาร
     3. subject: หัวเรื่อง
     4. to: เรียน
     5. attach: สิ่งที่ส่งมาด้วย
     6. detail: เนื้อความในหนังสือ
     7. signed_by: ผู้ลงนาม
     8. signed_by2: ตำแหน่งผู้ลงนาม
     9. contact: ช่องทางติดต่อ (เช่น เบอร์โทร อีเมล)
     10. download_url: ลิงก์สำหรับดาวน์โหลด (ถ้ามี)

     OCR_TEXT:
     """
     {{ $json["stdout"] }}
     """
     ```  
   - Connect output of "Extract Text with Typhoon OCR" to this node.  
   - Configure to use "OpenRouter Chat Model" as the LLM provider.

6. **Create Code Node**  
   - Type: Code (JavaScript)  
   - Name: "Parse JSON to Sheet Format"  
   - JS Code:  
     ```js
     const raw = $json["text"];

     // Remove possible markdown JSON code blocks
     const cleaned = raw.replace(/```json\n?|```/g, "").trim();

     let parsed;
     try {
       parsed = JSON.parse(cleaned);
     } catch (err) {
       throw new Error("JSON parsing failed: " + err.message + "\n\nRaw text:\n" + cleaned);
     }

     const contact = parsed.contact || {};

     return {
       book_id: parsed.book_id || "",
       date: parsed.date || "",
       subject: parsed.subject || "",
       to: parsed.to || "",
       attach: parsed.attach || "",
       detail: parsed.detail || "",
       signed_by: parsed.signed_by || "",
       signed_by2: parsed.signed_by2 || "",
       contact_phone: contact.phone || "",
       contact_email: contact.email || "",
       contact_fax: contact.fax || "",
       download_url: parsed.download_url || ""
     };
     ```  
   - Connect output of "Structure Text to JSON with LLM" to this node.

7. **Create Google Sheets Node**  
   - Type: Google Sheets  
   - Name: "Save to Google Sheet"  
   - Operation: Append  
   - Document ID: Use your Google Sheets document ID  
   - Sheet Name: Use the sheet GID or name (e.g., `gid=0`)  
   - Columns mapping: Map each field from the code node output to corresponding sheet columns (book_id, date, subject, to, attach, detail, signed_by, signed_by2, contact_phone, contact_email, contact_fax, download_url)  
   - Credentials: Configure Google Sheets OAuth2 credentials  
   - Connect output of "Parse JSON to Sheet Format" to this node.

8. **Optional: Create Sticky Note**  
   - Add a sticky note at the workflow start for documentation:  
     ```
     ## Thai OCR to Sheet
     This workflow extracts Thai PDF text using typhoon-ocr, converts it to structured JSON using LLM, and saves the output to Google Sheets. Works with self-hosted n8n only.
     ```

9. **Final Connection Map:**  
   - Manual Trigger → Load PDFs from doc Folder → Extract Text with Typhoon OCR → Structure Text to JSON with LLM → Parse JSON to Sheet Format → Save to Google Sheet

---

### 5. General Notes & Resources

| Note Content                                                                                                           | Context or Link                                                                                             |
|------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| This workflow requires a self-hosted n8n instance due to the use of Execute Command and local file access.              | Workflow operation environment                                                                               |
| Typhoon OCR API key must be kept secure and set correctly in the Execute Command node environment variable.             | https://typhoonocr.com/                                                                                      |
| OpenRouter API credentials are needed for Llama 3.1 model access.                                                       | https://openrouter.ai/                                                                                       |
| Google Sheets OAuth2 credentials must have permissions to append to the target spreadsheet.                             | https://developers.google.com/workspace/guides/create-credentials                                            |
| The JavaScript code node includes error handling for malformed JSON from the AI output; failed parsing will stop workflow.| Important for troubleshooting and ensuring data integrity                                                  |
| The prompt instructs the AI to output JSON in a specific format, which is essential for downstream parsing.             | Prompt engineering is key to robust data extraction                                                         |
| The workflow’s sticky note summarizes its purpose for easy identification in n8n editors.                              |                                                                                                            |

---

This document provides a comprehensive understanding of the "Extract and Structure Thai Documents to Google Sheets using Typhoon OCR and Llama 3.1" workflow, enabling detailed inspection, modification, or recreation by both advanced users and automation agents.

---

**Disclaimer:** The provided text originates exclusively from an automated workflow created with n8n, respecting content policies and handling only legal and public data.