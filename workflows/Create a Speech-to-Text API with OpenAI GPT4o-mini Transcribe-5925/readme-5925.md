Create a Speech-to-Text API with OpenAI GPT4o-mini Transcribe

https://n8nworkflows.xyz/workflows/create-a-speech-to-text-api-with-openai-gpt4o-mini-transcribe-5925


# Create a Speech-to-Text API with OpenAI GPT4o-mini Transcribe

---
### 1. Workflow Overview

This workflow implements a Speech-to-Text API using OpenAI’s GPT4o-mini transcription model. It is designed to receive audio files via a webhook, transcribe the audio into text using OpenAI’s API, and respond to the caller with the transcript. The workflow includes an example frontend interface (provided as HTML/CSS/JS) for recording and submitting audio to the webhook endpoint.

The workflow logic is structured into these main blocks:

- **1.1 Input Reception:** Receives incoming audio files through a webhook node configured to accept POST requests.
- **1.2 AI Processing:** Sends the received audio file to OpenAI’s GPT4o-mini transcription endpoint via an HTTP Request node.
- **1.3 Transcript Extraction:** Extracts the transcribed text from the OpenAI response and sets it into a structured JSON field.
- **1.4 Response Delivery:** Returns the transcript JSON back as the HTTP response to the webhook caller.
- **1.5 Frontend Interface (Documentation):** Provides HTML/JS code as a sticky note, demonstrating how to record audio in the browser and send it to the webhook for transcription.

---

### 2. Block-by-Block Analysis

#### 2.1 Input Reception

- **Overview:**  
  This block exposes a webhook endpoint that receives audio files uploaded via HTTP POST. It serves as the entry point for audio data into the workflow.

- **Nodes Involved:**  
  - `Webhook containing audio to transcribe`

- **Node Details:**  
  - **Node:** Webhook containing audio to transcribe  
  - **Type & Role:** Webhook node; listens for HTTP POST requests on path "/audio-to-transcribe" and accepts multipart form data containing the audio file.  
  - **Configuration:**  
    - HTTP Method: POST  
    - Path: "audio-to-transcribe"  
    - Response Mode: Wait for response node output before responding  
    - Webhook ID: autogenerated unique identifier  
  - **Inputs:** Incoming HTTP requests with audio file as multipart form-data  
  - **Outputs:** Passes received data to next node (HTTP Request node)  
  - **Edge Cases / Potential Failures:**  
    - Missing or malformed audio file input  
    - Invalid HTTP method or path  
    - Timeout or network errors before forwarding data  
  - **Notes:**  
    - The node expects the frontend or client to post the audio file under the form key `"audio_file"`.  
    - Sticky note nearby instructs users on the webhook usage and how to obtain the production URL for integration.

#### 2.2 AI Processing

- **Overview:**  
  This block sends the incoming audio file to OpenAI’s transcription API endpoint and retrieves the transcription result.

- **Nodes Involved:**  
  - `Transcribe with OpenAI`

- **Node Details:**  
  - **Node:** Transcribe with OpenAI  
  - **Type & Role:** HTTP Request node; sends multipart/form-data POST requests to OpenAI’s audio transcription endpoint.  
  - **Configuration:**  
    - URL: `https://api.openai.com/v1/audio/transcriptions`  
    - Method: POST  
    - Content Type: multipart-form-data  
    - Authentication: Uses predefined OpenAI API credentials (configured in n8n)  
    - Body Parameters:  
      - `file`: binds to incoming binary data field `"audio_file"` (expected audio data)  
      - `model`: fixed string `"gpt-4o-mini-transcribe"` specifying the transcription model  
  - **Inputs:** Receives audio file binary data passed from webhook node  
  - **Outputs:** Returns JSON response from OpenAI containing transcription text  
  - **Edge Cases / Potential Failures:**  
    - Invalid or expired OpenAI API key (authentication error)  
    - Unsupported audio file format or corrupted file  
    - API rate limits or network timeouts  
    - Model name changes or API deprecations  
  - **Version Requirements:** Uses HTTP Request node version 4.2 which supports multipart form data with binary inputs  
  - **Credentials:** Requires OpenAI API key set up in n8n credentials under `openAiApi`

#### 2.3 Transcript Extraction

- **Overview:**  
  This block extracts the transcription string from the OpenAI API JSON response and formats it for returning to the client.

- **Nodes Involved:**  
  - `Extract transcript`

- **Node Details:**  
  - **Node:** Extract transcript  
  - **Type & Role:** Set node; used to assign extracted transcription text to a named field `Transcript`.  
  - **Configuration:**  
    - Creates a new field named `"Transcript"` of type string  
    - Expression: `={{ $json.text }}` extracts the `"text"` field from the OpenAI response JSON  
  - **Inputs:** Receives JSON output from the HTTP Request node  
  - **Outputs:** Passes structured JSON with the `Transcript` field to the response node  
  - **Edge Cases / Potential Failures:**  
    - Missing or empty `"text"` field in API response  
    - Expression evaluation errors if JSON structure changes

#### 2.4 Response Delivery

- **Overview:**  
  This block sends the extracted transcript back as the HTTP response to the original webhook request, completing the API call.

- **Nodes Involved:**  
  - `Respond to Webhook with transcript`

- **Node Details:**  
  - **Node:** Respond to Webhook with transcript  
  - **Type & Role:** Respond to Webhook node; sends HTTP 200 response containing the transcript JSON back to the caller.  
  - **Configuration:**  
    - Response Code: 200 (OK)  
    - Automatically uses the input JSON data as the response body  
  - **Inputs:** Accepts the JSON object with `Transcript` field from the Set node  
  - **Outputs:** None (terminates workflow response)  
  - **Edge Cases / Potential Failures:**  
    - Malformed JSON or missing expected fields may cause client-side errors  
    - Timeout or network failure sending response  

#### 2.5 Frontend Interface (Documentation & Example Code)

- **Overview:**  
  This block contains detailed instructions and a complete HTML/JavaScript frontend example to record audio in the browser, submit it to the webhook, and display the transcript.

- **Nodes Involved:**  
  - `Sticky Note` (with full HTML/JS code)  
  - `Sticky Note1` through `Sticky Note5` (documentation and guidance notes)

- **Node Details:**  
  - **Sticky Note nodes:**  
    - Provide instructions on:  
      - Adding credentials for OpenAI API key  
      - Obtaining webhook URL to use as endpoint in the frontend  
      - How to use the provided HTML file  
      - How the frontend interacts with the webhook (posting audio as `audio_file`)  
      - Explanation of the AI transcription model used  
      - Response format expectations (JSON key `"Transcript"`)  
    - The main sticky note contains a full browser-based audio recorder interface with features:  
      - Start/Stop audio recording using MediaRecorder API  
      - Display recording timer  
      - Playback recorded audio  
      - Submit audio via Fetch API as multipart/form-data to the webhook URL (placeholder must be replaced with actual webhook URL)  
      - Show loading overlay while transcription is in progress  
      - Display transcript text on completion  
      - Allow re-recording and transcribing another audio clip  
  - **Edge Cases / Considerations:**  
    - The frontend uses browser APIs that require HTTPS and user microphone permissions  
    - The placeholder `YOUR WEBHOOK URL` must be replaced manually by the integrator  
    - Simple error handling in frontend alerts for microphone access denial or transcription failure  
    - Audio format recorded is `audio/webm` which should be supported by OpenAI’s transcription API  

---

### 3. Summary Table

| Node Name                     | Node Type                 | Functional Role                         | Input Node(s)                      | Output Node(s)                      | Sticky Note                                                                                                        |
|-------------------------------|---------------------------|---------------------------------------|----------------------------------|-----------------------------------|-------------------------------------------------------------------------------------------------------------------|
| Webhook containing audio to transcribe | Webhook                  | Receives audio file POST requests     |                                  | Transcribe with OpenAI             | ## The webhook to call from your app POST the audio as "audio_file" to this webhook to start the workflow.       |
| Transcribe with OpenAI          | HTTP Request             | Sends audio to OpenAI transcription API | Webhook containing audio to transcribe | Extract transcript                | ## AI transcription with OpenAI GPT4o-mini transcribe                                                             |
| Extract transcript              | Set                      | Extracts transcript text from API response | Transcribe with OpenAI             | Respond to Webhook with transcript |                                                                                                                   |
| Respond to Webhook with transcript | Respond to Webhook       | Sends transcription JSON response back | Extract transcript                |                                   | ## Sending the transcript back to your app Your app should expect the key "Transcript" in the body of the webhook response. |
| Sticky Note                    | Sticky Note              | Frontend interface HTML/JS code        |                                  |                                   | Complete functional audio recording and transcription frontend example, with UI and detailed instructions.       |
| Sticky Note1                   | Sticky Note              | Workflow usage instructions             |                                  |                                   | ## Speech Transcription API Endpoint How to use and setup instructions.                                           |
| Sticky Note2                   | Sticky Note              | Frontend example usage instructions     |                                  |                                   | ## Example Frontend Code Below Instructions on saving and using the provided HTML file.                           |
| Sticky Note3                   | Sticky Note              | Webhook call instructions                |                                  |                                   | ## The webhook to call from your app POST the audio as "audio_file" to this webhook to start the workflow.       |
| Sticky Note4                   | Sticky Note              | AI transcription model info              |                                  |                                   | ## AI transcription with OpenAI GPT4o-mini transcribe                                                             |
| Sticky Note5                   | Sticky Note              | Response format explanation               |                                  |                                   | ## Sending the transcript back to your app Your app should expect the key "Transcript" in the body of the webhook response. |

---

### 4. Reproducing the Workflow from Scratch

1. **Create the Webhook Node:**
   - Add a **Webhook** node named `"Webhook containing audio to transcribe"`.
   - Configure:
     - HTTP Method: `POST`
     - Path: `audio-to-transcribe`
     - Response Mode: `Response Node`
   - Save and activate the workflow after completion to obtain the Production URL.
   
2. **Create the HTTP Request Node:**
   - Add an **HTTP Request** node named `"Transcribe with OpenAI"`.
   - Configure:
     - URL: `https://api.openai.com/v1/audio/transcriptions`
     - Method: `POST`
     - Authentication: Select or create OpenAI API credentials (`openAiApi`).
     - Content Type: `multipart-form-data`
     - Body Parameters:
       - Add parameter with:
         - Name: `file`
         - Type: `Form Binary Data`
         - Input Data Field Name: `audio_file` (this must match the binary property passed from the webhook)
       - Add parameter:
         - Name: `model`
         - Value: `gpt-4o-mini-transcribe`
   - Connect the output of the webhook node to this node.
   
3. **Create the Set Node:**
   - Add a **Set** node named `"Extract transcript"`.
   - Configure:
     - Add a new field:
       - Name: `Transcript`
       - Type: `String`
       - Value: Expression `{{$json["text"]}}`
   - Connect the output of `"Transcribe with OpenAI"` node to this node.
   
4. **Create the Respond to Webhook Node:**
   - Add a **Respond to Webhook** node named `"Respond to Webhook with transcript"`.
   - Configure:
     - Response Code: 200
     - The node will automatically send the input JSON as the response body.
   - Connect the output of `"Extract transcript"` node to this node.
   
5. **Connect Nodes:**
   - Connect nodes in order:  
     `Webhook containing audio to transcribe` → `Transcribe with OpenAI` → `Extract transcript` → `Respond to Webhook with transcript`
   
6. **Credentials Setup:**
   - Add your OpenAI API key in n8n credentials under the type `openAiApi`.
   - Assign this credential to the `"Transcribe with OpenAI"` HTTP Request node.
   
7. **Frontend Setup:**
   - Create sticky notes or external documentation with:
     - Instructions to copy the Production URL of the webhook node.
     - Example frontend HTML/JavaScript code to record audio and send it as `audio_file` field to the webhook URL.  
     - In the frontend code, replace the placeholder `"YOUR WEBHOOK URL"` with your actual webhook endpoint URL.
   
8. **Testing:**
   - Activate the workflow.
   - Open the example HTML frontend in a modern browser (supports MediaRecorder API).
   - Record audio and submit to the webhook.
   - Verify the transcript is returned and displayed.
   
---

### 5. General Notes & Resources

| Note Content                                                                                                                                | Context or Link                                                                                                  |
|---------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| This workflow exposes a simple Speech-to-Text API endpoint using OpenAI’s GPT4o-mini transcription model.                                   | Workflow purpose                                                                                                 |
| The frontend example uses modern browser APIs (MediaRecorder, Fetch) and Tailwind CSS for styling.                                          | Frontend interface styling and functionality                                                                     |
| Replace the placeholder `"YOUR WEBHOOK URL"` in the frontend code with your actual webhook Production URL after activating the workflow.    | Critical for frontend integration                                                                                 |
| The workflow expects audio in `audio/webm` or other formats supported by OpenAI's transcription endpoint.                                   | Audio format compatibility                                                                                        |
| OpenAI API rate limits and authentication failures are possible sources of runtime errors.                                                  | Operational considerations                                                                                        |
| The `Transcript` field is provided in the response JSON for client applications to consume easily.                                          | API response contract                                                                                             |
| For more details on OpenAI transcription models, refer to OpenAI official documentation: https://platform.openai.com/docs/api-reference/audio    | External reference                                                                                               |
| The frontend includes error handling for microphone access denial and audio submission failures.                                            | Frontend robustness                                                                                              |

---

**Disclaimer:**  
The text provided above is derived exclusively from an n8n automation workflow. All data and API usage comply strictly with applicable content policies and legal constraints. The workflow manipulates only public and legal data sources.